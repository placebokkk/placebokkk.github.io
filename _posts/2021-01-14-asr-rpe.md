---
layout: post
title:  "Relatvie Positional Embeding"
date:   2021-01-14 10:00:00 +0800
categories: asr
---

# Relative Positional Embedding

Transformer XL中提出了Relative Positional Embedding方法，在ASR conformer论文中，也提到

> We employ multi-headed self-attention (MHSA) while integrating an important  technique from Transformer-XL [20],the relative sinusoidal positional encoding scheme. The relative po- sitional encoding allows the self-attention module to generalize better on different input length and the resulting encoder is more robust to the variance of the utterance length.
```


## 原始的带positional embedding的 attention score计算方法
i位置的encoding embedding为$$\mathbf{E}_{x_{i}}$$,positional embedding信息$$\mathbf{U}_{k}$$, i和j位置的attention score计算如下:

$$
(\mathbf{E}_{x_{i}}^{\top}+\mathbf{U}_{i}^{\top}) \mathbf{W}_{q}^{\top} \mathbf{W}_{k} (\mathbf{E}_{x_{j}}+\mathbf{U}_{j})
$$


展开得到

$$
\begin{aligned}
\mathbf{A}_{i, j}^{\mathrm{abs}} &=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)} \\
&+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)} .
\end{aligned}
$$

去除$$\mathbf{W}_{q}$$和$$\mathbf{W}_{k}$$简化形式:

$$
\mathbf{A}_{i, j}^{\mathrm{abs}} =\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top}  \mathbf{U}_{j}}_{(b)}  +\underbrace{\mathbf{U}_{i}^{\top}  \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top}  \mathbf{U}_{j}}_{(d)} 
$$

## Relative Positional Embedding
相对位置这个概念，只有做attention时才存在，
Relative Positional表示使用$$R_{i-j}$$第i个位置的query和第j个位置的key做attention时用的相对距离信息:
* 第3个位置的query和第2个位置的key做attention，使用R1.
* 第4个位置的query和第3个位置的key做attention，使用R1.
* 第4个位置的query和第2个位置的key做attention，使用R2.

使用R_{i-j}代替，保证引入两个跟位置无关的可学习的向量参数u和v

$$
\mathbf{A}_{i, j}^{\mathrm{rel}} =\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top}  \mathbf{R}_{i-j}}_{(b)} +\underbrace{\mathbf{u}^{\top}  \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{v}^{\top}  \mathbf{R}_{i-j}}_{(d)} 
$$

合并ac项，bd项:

$$
\mathbf{A}_{i, j}^{\mathrm{rel}} =\underbrace{(\mathbf{E}_{x_{i}}^{\top}+\mathbf{u}^{\top}) \mathbf{E}_{x_{j}}}_{(ac)}+\underbrace{(\mathbf{E}_{x_{i}}^{\top}+\mathbf{v}^{\top})  \mathbf{R}_{i-j}}_{(bd)}
$$

Score matrix可以写成两项之和

$$
\mathbf{A}^{\mathrm{rel}} = \mathbf{A}_{ac} + \mathbf{A}_{bd}
$$

ac项容易计算，这里只考虑bd项, 这里为了表示方便，用$$R_{j-i}$$代替$$R_{i-j}$$, 同时令 $$\mathbf{q}_{i} = \mathbf{E}_{x_{i}} + \mathbf{v}$$

$$
\mathbf{A}_{bd}=\left[\begin{array}{cccc}
q_{0}^{\top} \mathbf{R}_{0} & q_{0}^{\top} \mathbf{R}_{1} & \cdots & q_{0}^{\top} \mathbf{R}_{L-1} \\
q_{1}^{\top} \mathbf{R}_{-1} & q_{1}^{\top} \mathbf{R}_{0} & \cdots & q_{1}^{\top} \mathbf{R}_{L-2} \\
\vdots & \vdots & \ddots  & \vdots \\
q_{L-1}^{\top} \mathbf{R}_{-(L-1)} & q_{L-1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{L-1}^{\top} \mathbf{R}_{0}
\end{array}\right]
$$

如何高效计算矩阵$$\mathbf{A}_{bd}$$ ? 

令

$$
\mathbf{R}=\left[\begin{array}{llllll}
\mathbf{R}_{-(L-1)} & \mathbf{R}_{-(L-2)} &  \mathbf{R}_{0} & \cdots & \mathbf{R}_{L-2} &  \mathbf{R}_{L-1}
\end{array}\right]
$$

$$
\mathbf{q}=\left[\begin{array}{c}
\mathbf{q}_{0}^{\top} \\
\mathbf{q}_{1}^{\top} \\
\vdots \\
\mathbf{q}_{L-1}^{\top} \\
\mathbf{q}_{L}^{\top}
\end{array}\right]
$$

两个矩阵相乘

$$
\mathbf{q} \mathbf{R}^{\top}=\left[\begin{array}{ccccccc}
q_{0}^{\top} \mathbf{R}_{-(L-1)} & q_{0}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{0}^{\top} \mathbf{R}_{0} & \cdots & q_{0}^{\top} \mathbf{R}_{L-2} & q_{0}^{\top} \mathbf{R}_{L-1} \\
q_{1}^{\top} \mathbf{R}_{-(L-1)} & q_{1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{1}^{\top} \mathbf{R}_{0} & \cdots & q_{1}^{\top} \mathbf{R}_{L-2} & q_{1}^{\top} \mathbf{R}_{L-1} \\
\vdots & \vdots & \ddots  & \vdots & \ddots & \vdots & \vdots \\
q_{L-1}^{\top} \mathbf{R}_{-(L-1)} & q_{L-1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{L-1}^{\top} \mathbf{R}_{0} & \cdots & q_{L-1}^{\top} \mathbf{R}_{L-2} & q_{L-1}^{\top} \mathbf{R}_{L-1}
\end{array}\right]
$$

对该矩阵，将第i行左移L-i个位置，并且取前L列即得到$$\mathbf{A}_{bd}$$

$$
\mathbf{q} \mathbf{R}^{\top}=\left[\begin{array}{cccccc}
q_{0}^{\top} \mathbf{R}_{0} & \cdots & q_{0}^{\top} \mathbf{R}_{M} & q_{0}^{\top} \mathbf{R}_{M+1} & \cdots & q_{0}^{\top} \mathbf{R}_{M+L-1} \\
q_{1}^{\top} \mathbf{R}_{0} & \cdots & q_{1}^{\top} \mathbf{R}_{M} & q_{1}^{\top} \mathbf{R}_{M+1} & \cdots & q_{1}^{\top} \mathbf{R}_{M+L-1} \\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
q_{L-1}^{\top} \mathbf{R}_{0} & \cdots & q_{L-1}^{\top} \mathbf{R}_{M} & q_{L-1}^{\top} \mathbf{R}_{M+1} & \cdots & q_{L-1}^{\top} \mathbf{R}_{M+L-1}
\end{array}\right]
$$


代码rel_shift，就是将第i行左移L-i个位置。