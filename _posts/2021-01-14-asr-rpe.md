---
layout: post
title:  "Relative Positional Embedding"
date:   2021-01-14 10:00:00 +0800
categories: asr
---

本文讲解基于transformer/conformer的语音识别Wenet中的Relative Positional Embed的实现

[Wenet代码](https://github.com/mobvoi/wenet/tree/main/wenet)


# Relative Positional Embedding

Transformer XL中提出了Relative Positional Embedding方法，在ASR conformer论文中，也提到

> We employ multi-headed self-attention (MHSA) while integrating an important  technique from Transformer-XL [20],the relative sinusoidal positional encoding scheme. The relative po- sitional encoding allows the self-attention module to generalize better on different input length and the resulting encoder is more robust to the variance of the utterance length.
```


## 原始的带positional embedding的 attention score计算方法
i位置的encoding embedding为$$\mathbf{E}_{x_{i}}$$,positional embedding信息$$\mathbf{U}_{k}$$, i和j位置的attention score计算如下:

$$
(\mathbf{E}_{x_{i}}^{\top}+\mathbf{U}_{i}^{\top}) \mathbf{W}_{q}^{\top} \mathbf{W}_{k} (\mathbf{E}_{x_{j}}+\mathbf{U}_{j})
$$


展开得到

$$
\begin{aligned}
\mathbf{A}_{i, j}^{\mathrm{abs}} &=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)} \\
&+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)} .
\end{aligned}
$$

去除$$\mathbf{W}_{q}$$和$$\mathbf{W}_{k}$$简化形式:

$$
\mathbf{A}_{i, j}^{\mathrm{abs}} =\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top}  \mathbf{U}_{j}}_{(b)}  +\underbrace{\mathbf{U}_{i}^{\top}  \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top}  \mathbf{U}_{j}}_{(d)} 
$$

## Relative Positional Embedding
相对位置这个概念，只有做attention时才存在，
Relative Positional表示使用$$R_{i-j}$$第i个位置的query和第j个位置的key做attention时用的相对距离信息:
* 第3个位置的query和第2个位置的key做attention，使用R1.
* 第4个位置的query和第3个位置的key做attention，使用R1.
* 第4个位置的query和第2个位置的key做attention，使用R2.

使用R_{i-j}代替，保证引入两个跟位置无关的可学习的向量参数u和v

$$
\mathbf{A}_{i, j}^{\mathrm{rel}} =\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top}  \mathbf{R}_{i-j}}_{(b)} +\underbrace{\mathbf{u}^{\top}  \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{v}^{\top}  \mathbf{R}_{i-j}}_{(d)} 
$$

合并ac项，bd项:

$$
\mathbf{A}_{i, j}^{\mathrm{rel}} =\underbrace{(\mathbf{E}_{x_{i}}^{\top}+\mathbf{u}^{\top}) \mathbf{E}_{x_{j}}}_{(ac)}+\underbrace{(\mathbf{E}_{x_{i}}^{\top}+\mathbf{v}^{\top})  \mathbf{R}_{i-j}}_{(bd)}
$$

Score matrix可以写成两项之和

$$
\mathbf{A}^{\mathrm{rel}} = \mathbf{A}_{ac} + \mathbf{A}_{bd}
$$

ac项容易计算，这里只考虑bd项, 这里为了表示方便，用$$R_{j-i}$$代替$$R_{i-j}$$, 同时令 $$\mathbf{q}_{i} = \mathbf{E}_{x_{i}} + \mathbf{v}$$

$$
\mathbf{A}_{bd}=\left[\begin{array}{cccc}
q_{0}^{\top} \mathbf{R}_{0} & q_{0}^{\top} \mathbf{R}_{1} & \cdots & q_{0}^{\top} \mathbf{R}_{L-1} \\
q_{1}^{\top} \mathbf{R}_{-1} & q_{1}^{\top} \mathbf{R}_{0} & \cdots & q_{1}^{\top} \mathbf{R}_{L-2} \\
\vdots & \vdots & \ddots  & \vdots \\
q_{L-1}^{\top} \mathbf{R}_{-(L-1)} & q_{L-1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{L-1}^{\top} \mathbf{R}_{0}
\end{array}\right]
$$

如何高效计算矩阵$$\mathbf{A}_{bd}$$ ? 

令

$$
\mathbf{R}=\left[\begin{array}{llllll}
\mathbf{R}_{-(L-1)} & \mathbf{R}_{-(L-2)} &  \mathbf{R}_{0} & \cdots & \mathbf{R}_{L-2} &  \mathbf{R}_{L-1}
\end{array}\right]
$$

$$
\mathbf{q}=\left[\begin{array}{c}
\mathbf{q}_{0}^{\top} \\
\mathbf{q}_{1}^{\top} \\
\vdots \\
\mathbf{q}_{L-1}^{\top} \\
\mathbf{q}_{L}^{\top}
\end{array}\right]
$$

两个矩阵相乘

$$
\mathbf{q} \mathbf{R}^{\top}=\left[\begin{array}{ccccccc}
q_{0}^{\top} \mathbf{R}_{-(L-1)} & q_{0}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{0}^{\top} \mathbf{R}_{0} & \cdots & q_{0}^{\top} \mathbf{R}_{L-2} & q_{0}^{\top} \mathbf{R}_{L-1} \\
q_{1}^{\top} \mathbf{R}_{-(L-1)} & q_{1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{1}^{\top} \mathbf{R}_{0} & \cdots & q_{1}^{\top} \mathbf{R}_{L-2} & q_{1}^{\top} \mathbf{R}_{L-1} \\
\vdots & \vdots & \ddots  & \vdots & \ddots & \vdots & \vdots \\
q_{L-1}^{\top} \mathbf{R}_{-(L-1)} & q_{L-1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{L-1}^{\top} \mathbf{R}_{0} & \cdots & q_{L-1}^{\top} \mathbf{R}_{L-2} & q_{L-1}^{\top} \mathbf{R}_{L-1}
\end{array}\right]
$$

对该矩阵，将第i行左移L-i个位置，并且取前L列即得到$$\mathbf{A}_{bd}$$

$$
\mathbf{A}_{bd}=\left[\begin{array}{cccc}
q_{0}^{\top} \mathbf{R}_{0} & q_{0}^{\top} \mathbf{R}_{1} & \cdots & q_{0}^{\top} \mathbf{R}_{L-1} \\
q_{1}^{\top} \mathbf{R}_{-1} & q_{1}^{\top} \mathbf{R}_{0} & \cdots & q_{1}^{\top} \mathbf{R}_{L-2} \\
\vdots & \vdots & \ddots  & \vdots \\
q_{L-1}^{\top} \mathbf{R}_{-(L-1)} & q_{L-1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{L-1}^{\top} \mathbf{R}_{0}
\end{array}\right]
$$

以上是一个full-attention的情况，在transformer-XL中，一个长度为L的segment要和他自身以前之前长度为
M的memory做attnetion。 在chunk-based的ASR也是类似的，当前chunk内的帧要和之前帧以及本chunk内的帧做attention。假设当前chunk为C，
之前帧数为M，则对于每个chunk，query的个数为C个，key/value的个数为M+C个。

令L=M+C，则一个chunk的score matrix计算方式如下

$$
\mathbf{A}_{bd}=\left[\begin{array}{cccc}
q_{0}^{\top} \mathbf{R}_{L-C} & q_{0}^{\top} \mathbf{R}_{L-C+1} & \cdots & q_{0}^{\top} \mathbf{R}_{L-1} \\
q_{1}^{\top} \mathbf{R}_{L-C-1} & q_{1}^{\top} \mathbf{R}_{L-C} & \cdots & q_{1}^{\top} \mathbf{R}_{L-2} \\
\vdots & \vdots & \ddots  & \vdots \\
q_{C-1}^{\top} \mathbf{R}_{-(L-1)} & q_{L-1}^{\top} \mathbf{R}_{-(L-2)} & \cdots & q_{L-1}^{\top} \mathbf{R}_{0}
\end{array}\right]
$$


## 代码实现

### 产生Relative Positional Embedding

生成一个[-L,L]的Relative Positional Embedding的方法:

```
class RelPositionalEncoding():
    def __init__(self,
                    d_model: int,
                    dropout_rate: float,
                    max_len: int = 5000):
            super().__init__()
            self.d_model = d_model
            self.max_len = max_len

            self.pe = torch.zeros(self.max_len, self.d_model)
            position = torch.arange(0, self.max_len,
                                    dtype=torch.float32).unsqueeze(1)
            div_term = torch.exp(
                torch.arange(0, self.d_model, 2, dtype=torch.float32) *
                -(math.log(10000.0) / self.d_model))
            self.pe[:, 0::2] = torch.sin(position * div_term)
            self.pe[:, 1::2] = torch.cos(position * div_term)

    def position_encoding(self, size: int) -> torch.Tensor:
        assert size < int(self.max_len/2)
        mid_pos = int(self.max_len / 2)
        pos_emb = self.pe[mid_pos - size + 1:mid_pos + size]
        return pos_emb
```
### attention 计算
利用生成2*L-1长度的Relative Positional Embedding和query embeddings做矩阵乘法，
计算方式和Multi-head attention类似

```
def forward(self, query: torch.Tensor, key: torch.Tensor,
                value: torch.Tensor, pos_emb: torch.Tensor,
                mask: Optional[torch.Tensor]):
        """Compute 'Scaled Dot Product Attention' with rel. positional encoding.
        Args:
            query (torch.Tensor): Query tensor (#batch, time1, size).
            key (torch.Tensor): Key tensor (#batch, time2, size).
            value (torch.Tensor): Value tensor (#batch, time2, size).
            pos_emb (torch.Tensor): Positional embedding tensor
                (2*time2-1, size).
            mask (torch.Tensor): Mask tensor (#batch, time1, time2).
        Returns:
            torch.Tensor: Output tensor (#batch, time1, d_model).
        """
        q, k, v = self.forward_qkv(query, key, value)
        q = q.transpose(1, 2)  # (batch, time1, head, d_k)

        p = self.linear_pos(pos_emb).view(-1, self.h, self.d_k)  # (2*time2-1, head, d_k)
        p = p.transpose(0, 1)  # (head, 2*time2-1, d_k)

        # (batch, head, time1, d_k)
        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)
        # (batch, head, time1, d_k)
        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)

        # compute attention score
        # first compute matrix a and matrix c
        # as described in https://arxiv.org/abs/1901.02860 Section 3.3
        # (batch, head, time1, time2)
        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))

        # compute matrix b and matrix d
        # (batch, head, time1, 2*time2 -1)
        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
        # Remove rel_shift since it is useless in speech recognition,
        # and it requires special attention for streaming.
        matrix_bd = self.rel_shift(matrix_bd)
        matrix_bd = matrix_bd[:, :, :, :matrix_ac.size(3)]
        scores = (matrix_ac + matrix_bd) / math.sqrt(
            self.d_k)  # (batch, head, time1, time2)

        return self.forward_attention(v, scores, mask)
```

matrix_bd的计算:

```
p = self.linear_pos(pos_emb).view(-1, self.h, self.d_k)  # (2*time2-1, head, d_k)
p = p.transpose(0, 1)  # (head, 2*time2-1, d_k)
matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
matrix_bd = self.rel_shift(matrix_bd)
matrix_bd = matrix_bd[:, :, :, :matrix_ac.size(3)]
```


### 矩阵左移操作

代码rel_shift，就是将第i行左移L-i个位置。


    def rel_shift(self, x):
        """Compute relative positinal encoding.
        Args:
            x (torch.Tensor): Input tensor (batch, head, time1, time2)
            (batch, time, size).
            zero_triu (bool): If true, return the lower triangular part of
                the matrix.
        Returns:
            torch.Tensor: Output tensor.
        """
        zero_pad = torch.zeros((x.size(0), x.size(1), x.size(2), 1),
                               device=x.device,
                               dtype=x.dtype)
        x_padded = torch.cat([zero_pad, x], dim=-1)

        x_padded = x_padded.view(x.size(0),
                                 x.size(1),
                                 x.size(3) + 1, x.size(2))
        x = x_padded[:, :, 1:].view_as(x)
        return x
```

下面以C=3,L=4举例，

对于一个C x (2L-1)的矩阵，总共用C*(2L-1)个元素，
```
tensor([[[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11., 12., 13., 14.],
          [ 15., 16., 17., 18., 19., 20., 21.]]]])
```

每一行开头增加一个0，
```
tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
          [ 0.,  8.,  9., 10., 11., 12., 13., 14.],
          [ 0., 15., 16., 17., 18., 19., 20., 21.]]]])
```

变为C*(2L-1)+C个元素，然后将行列展开成一行，
```
tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  0.,  8.,  9., 10., 11., 12.,
        13., 14.,  0., 15., 16., 17., 18., 19., 20., 21.])
```

去除开头的C个元素，
```
tensor([ 3.,  4.,  5.,  6.,  7.,  0.,  8.,  9., 10., 11., 12., 13., 14.,  0.,
        15., 16., 17., 18., 19., 20., 21.])
```

再排列成C x (2L-1)的矩阵。
```
tensor([[[[ 3.,  4.,  5.,  6.,  7.,  0.,  8.],
          [ 9., 10., 11., 12., 13., 14.,  0.],
          [15., 16., 17., 18., 19., 20., 21.]]]])
```

取前L列
```
tensor([[[[ 3.,  4.,  5.],
          [ 9., 10., 11.],
          [15., 16., 17.]]]])
```
